{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "zL2OMUdyPus3",
   "metadata": {
    "id": "zL2OMUdyPus3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "        ======================================================   MAIN PIPELINE MODULE   ==================================================\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "This is the primary pipeline script for the project. It orchestrates the end-to-end analysis workflow, serving as the central entry point where all\n",
    "key processes, configuration parameters, and workflow logic are put together.\n",
    "\n",
    "Key responsibilities include:\n",
    "- Importing project configuration settings (hyperparameters), utility routines, and clustering and statistical analysis modules.\n",
    "- Loading, merging, and caching clinical cohort and lab event datasets to optimize performance.\n",
    "- Preprocessing data by filtering common labs based on coverage thresholds, performing temporal discretization, normalization, interpolation,\n",
    "  and imputation to produce feature vectors in multiple formats (unimputed pivot vector, numeric arrays, fully imputed feature vector).\n",
    "- Managing a modular, flag-driven execution framework that dynamically invokes diverse clustering algorithms (e.g., KMeans, Agglomerative with\n",
    "  various distance metrics, Binary distances, DBSCAN, Spectral, KMedoids) based on configurable analysis flags.\n",
    "- Computing, caching, and loading precomputed distance matrices and intermediate outputs for computational efficiency and reproducibility.\n",
    "- Evaluating clustering results with multiple metrics (Silhouette score, Calinski-Harabasz, Davies-Bouldin indices) and conducting statistical\n",
    "  significance testing (e.g., Fisher's Exact Test) on cluster associations with clinical outcomes.\n",
    "- Saving all key outputs—feature matrices, clustering metrics plots, distance matrices, and test results—in structured folders to maintain\n",
    "  project organization.\n",
    "- Logging all major processing steps, runtime metrics, and results systematically to support transparency, auditability, and collaborative\n",
    "  development.\n",
    "- Supporting interactive iterative development through module reloading to apply code updates without restarting the environment.\n",
    "\n",
    "This design enables reproducible, transparent, and extensible experiments with standardized data processing, multiple clustering techniques,\n",
    "and rigorous evaluation, forming a robust foundation for clinical data-driven phenotyping and analysis.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#   ================================================================= IMPORTS AND CONFIGURATION ============================================================================\n",
    "\n",
    "from hyperparameters import *         # Import all analysis hyperparameters and pipeline flags\n",
    "import utils                          # Import all utility functions (data loading, filtering, etc.)\n",
    "from imports import *                 # Import packages (pandas, numpy, etc.)\n",
    "from clustering import *              # Import clustering method implementations\n",
    "import re\n",
    "\n",
    "utils.configure_logging()             # Set up standardized logging format for the pipeline\n",
    "logger = logging.getLogger('MAIN')\n",
    "\n",
    "\n",
    "#   ====================================================================== DATA LOADING ====================================================================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load or cache cohort and labevents dataframes.\n",
    "    Cached .pkl files are loaded if present, otherwise original CSVs are loaded and cached for the future.\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    if utils.check_existing_raw_data():\n",
    "        cohort_data = pd.read_pickle(os.path.join(utils.output_folder, 'cohort_data.pkl'))\n",
    "        labevents_data = pd.read_pickle(os.path.join(utils.output_folder, 'labevents_data.pkl'))\n",
    "\n",
    "    else:\n",
    "        # These paths should be adapted if data is relocated\n",
    "        data_path_cohort = \"C:/Ahad/Project/data/cohort1_target.csv\"         # Adjust path as needed\n",
    "        data_path_labevent = \"C:/Ahad/Project/data/labevents.csv\"            # Adjust path as needed\n",
    "        cohort_data = utils.loading_data(data_path_cohort,  'dischtime')\n",
    "        labevents_data = utils.loading_data(data_path_labevent, 'charttime')\n",
    "        utils.save_raw_dataframes(cohort_data, labevents_data)\n",
    "        elapsed_time = time.time() - start\n",
    "        logger.info(\"Execution time to load the data: %d minutes and %d seconds\",\n",
    "                    int(elapsed_time // 60), int(elapsed_time % 60))\n",
    "\n",
    "    # Quick logging of the data head for sanity check \n",
    "    logger.info(f\"Cohort data:\\n{cohort_data.head()}\\n\")\n",
    "    logger.info(f\"Lab events data:\\n{labevents_data.head()}\\n\")\n",
    "\n",
    "    return cohort_data, labevents_data\n",
    "\n",
    "\n",
    "#    ====================================================================== DATA PREPROCESSING =========================================================================\n",
    "\n",
    "\n",
    "def preprocess(cohort_data, labevents_data, set_percentage, days, PCT_LABEL):\n",
    "    \"\"\"\n",
    "    Preprocess, cache, and return the data needed for clustering.\n",
    "    Handles both cached and fresh computation, and saves results for future runs.\n",
    "    \"\"\"\n",
    "\n",
    "    if utils.check_existing_preprocessed_files(label=PCT_LABEL):\n",
    "\n",
    "        # Load from cache for speed/reproducibility\n",
    "        filtered_common_labs_percent = pd.read_pickle(os.path.join(utils.output_folder, f'filtered_common_labs_{PCT_LABEL}.pkl'))\n",
    "        pivot_percent = pd.read_pickle(os.path.join(utils.output_folder, f'unimputed_vector_{PCT_LABEL}.pkl'))\n",
    "        feature_matrix_percent = pd.read_pickle(os.path.join(utils.output_folder, f'feature_matrix_{PCT_LABEL}.pkl'))\n",
    "        unimputed_normalized = pd.read_pickle(os.path.join(utils.output_folder, f'unimputed_normalized_vector_{PCT_LABEL}.pkl'))\n",
    "        imputed_preprocessed = pd.read_pickle(os.path.join(utils.output_folder, f'imputed_vector_{PCT_LABEL}.pkl'))\n",
    "        id_vars = ['hadm_id', 'subject_id']\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Run full preprocessing pipeline if results are not found in cache\n",
    "        merged = utils.merge_data(labevents_data, cohort_data)\n",
    "        logger.info(\"Data merged successfully\")\n",
    "\n",
    "        result = utils.labs_within_n_days_of_discharge(merged, days)\n",
    "        logger.info(\"Filtered lab results are successfully taken within the last %d days.\\n\", days)\n",
    "\n",
    "        filtered_common_labs_percent, unique_labs, total_admissions = utils.filtering_labs_by_percentage(result, set_percentage, days)\n",
    "        pivot_percent = utils.descritization(filtered_common_labs_percent, days)\n",
    "        feature_matrix_percent, feature_cols_percent, id_vars = utils.get_feature_matrix(pivot_percent)\n",
    "        long_df = utils.create_long_format(pivot_percent, feature_cols_percent, id_vars)\n",
    "        unimputed_normalized = utils.prepare_unimputed_normalized(long_df, id_vars=['hadm_id', 'subject_id'])\n",
    "        normalized_long_df_percent = utils.create_long_format(pivot_percent, feature_cols_percent, id_vars)\n",
    "        imputed_df_percent, wide_df_percent = utils.get_imputed_df(normalized_long_df_percent, id_vars)\n",
    "        imputed_preprocessed = utils.knn_impute_and_sort_features(wide_df_percent, id_vars, days)\n",
    "        feature_matrix_df = pd.DataFrame(feature_matrix_percent, columns=feature_cols_percent)\n",
    "\n",
    "        utils.save_preprocessing_outputs(\n",
    "            filtered_common_labs_percent,           # Filtered common labs DataFrame\n",
    "            pivot_percent,                          # Unimputed vector                    \n",
    "            feature_matrix_df,                      # 2D NumPy array derived from pivot_percent vector   \n",
    "            unimputed_normalized,                   # Unimputed normalized vector             \n",
    "            imputed_preprocessed,                   # Imputed vector           \n",
    "            label=PCT_LABEL\n",
    "        )\n",
    "\n",
    "    return filtered_common_labs_percent, pivot_percent, feature_matrix_percent, unimputed_normalized, imputed_preprocessed, id_vars\n",
    "\n",
    "\n",
    "#    ==================================================================== CLUSTERING AND EVALUATION =======================================================================\n",
    "\n",
    "def run_clustering(filtered_common_labs_percent, pivot_percent, feature_matrix_percent, unimputed_normalized, imputed_preprocessed, id_vars, cohort_data, PCT_LABEL):\n",
    "\n",
    "    \"\"\"\n",
    "    Executes all clustering algorithms and evaluation logic based on analysis flags.\n",
    "    Handles caching of metrics, plotting, and statistical significance tests for clusters.\n",
    "    Metric plots are always saved in folder 'metric_results'.\n",
    "    \"\"\"\n",
    "\n",
    "# ==================================================\n",
    "#     K-MEANS Clustering using 'euclidean' metric\n",
    "# ==================================================\n",
    "\n",
    "    # Run standard KMeans clustering using 'euclidean' metric\n",
    "    if utils.should_run(\"KMEANS\", RUN_ALL, RUN_FLAGS):\n",
    "        metrics_df_km = run_kmeans_metrics(imputed_preprocessed, id_vars, K_RANGE, logger=logger)\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        Kmeans_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Kmeans_clustering_imputed\")\n",
    "        os.makedirs(Kmeans_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_km,\n",
    "            filename=\"Kmeans_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=Kmeans_dir\n",
    "        )\n",
    "\n",
    "        metrics_df_km.to_csv(\n",
    "            os.path.join(Kmeans_dir, \"Kmeans_metrics_score.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_km = get_top_clusters_with_threshold(metrics_df_km, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_km = utils.run_fisher_loop(\n",
    "            top_clusters=top_clust_km,\n",
    "            fisher_fn=run_fisher_test,\n",
    "            base_kwargs=dict(\n",
    "                df_features=imputed_preprocessed,           # Imputed vector   \n",
    "                id_vars=id_vars,\n",
    "                outcome_df=cohort_data,\n",
    "                outcome_col='target',\n",
    "                random_state=42,\n",
    "                n_init=10\n",
    "            ),\n",
    "            logger=logger,\n",
    "            method_label=f\"KMeans-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_km.items():\n",
    "            fisher_csv_path = os.path.join(\n",
    "                Kmeans_dir,\n",
    "                f\"Kmeans_fisher_test_{k}.csv\"\n",
    "            )\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_km = top_clust_km = all_fisher_km = None\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "#    Agglomerative Clustering (Ward) using 'euclidean' metric\n",
    "# ==============================================================\n",
    "\n",
    "    # Run Agglomerative clustering (Ward/euclidean) using 'euclidean' metric\n",
    "    if utils.should_run(\"AGG_EUCLID\", RUN_ALL, RUN_FLAGS):\n",
    "        metrics_df_agg_eu = run_agglomerative_metrics_euclidean(imputed_preprocessed, id_vars, K_RANGE, logger=logger)\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        agg_ward_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_clustering_ward_imputed\")\n",
    "        os.makedirs(agg_ward_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_eu,\n",
    "            filename=\"Agglomerative_ward_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=agg_ward_dir\n",
    "        )\n",
    "\n",
    "        metrics_df_agg_eu.to_csv(\n",
    "            os.path.join(agg_ward_dir, \"Agglomerative_ward_metrics_score.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_eu = get_top_clusters_with_threshold(metrics_df_agg_eu, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_eu = utils.run_fisher_loop(\n",
    "            top_clusters=top_clust_agg_eu,\n",
    "            fisher_fn=run_fisher_test_agglomerative_euclidean,\n",
    "            base_kwargs=dict(\n",
    "                df_features=imputed_preprocessed,           # Imputed vector\n",
    "                id_vars=id_vars,\n",
    "                outcome_df=cohort_data,\n",
    "                outcome_col=\"target\",\n",
    "                linkage=\"ward\",\n",
    "                metric=\"euclidean\"\n",
    "            ),\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Euclidean-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_eu.items():\n",
    "            fisher_csv_path = os.path.join(\n",
    "                agg_ward_dir,\n",
    "                f\"Agglomerative_ward_fisher_test_{k}.csv\"\n",
    "            )\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_eu = top_clust_agg_eu = all_fisher_agg_eu = None\n",
    "\n",
    "\n",
    "# ==================================================================================\n",
    "#    Agglomerative Clustering using Manhattan Distance matrix (Unimputed vector)\n",
    "# ==================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Manhattan distance matrix using unimputed vector\n",
    "    if utils.should_run(\"AGG_MANHATTAN_UNIMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_man, dist_man = run_agglomerative_metrics_manhattan(\n",
    "            df_raw=pivot_percent,           # Unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        manhattan_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_manhattan_clustering_unimputed\")\n",
    "        os.makedirs(manhattan_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_man,\n",
    "            filename=\"Manhattan_unimputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=manhattan_dir\n",
    "        )\n",
    "\n",
    "        metrics_df_agg_man.to_csv(\n",
    "            os.path.join(manhattan_dir, \"Manhattan_unimputed_metrics_score.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_man = get_top_clusters_with_threshold(metrics_df_agg_man, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_man = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_man,\n",
    "            fisher_fn=run_fisher_test_agglomerative_manhattan,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_man,\n",
    "            df_ids=pivot_percent[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Manhattan-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_man.items():\n",
    "            fisher_csv_path = os.path.join(\n",
    "                manhattan_dir,\n",
    "                f\"Manhattan_unimputed_fisher_test_{k}.csv\"\n",
    "            )\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_man = dist_man = top_clust_agg_man = all_fisher_agg_man = None\n",
    "\n",
    "\n",
    "# ============================================================================================\n",
    "#    Agglomerative Clustering using Manhattan Distance matrix (Unimputed normalized vector)\n",
    "# ============================================================================================\n",
    "    \n",
    "    # Run Agglomerative clustering with precomputed Manhattan distance matrix using normalized unimputed vector\n",
    "    if utils.should_run(\"AGG_MANHATTAN_UNIMPUTED_NORMALIZED\", RUN_ALL, RUN_FLAGS):\n",
    "    \n",
    "        metrics_df_agg_man_norm, dist_man_norm = run_agglomerative_metrics_manhattan_unimputed_normalized(\n",
    "            df_raw=unimputed_normalized,           # Normalized unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "    \n",
    "        # Create per-method/per-threshold output directory\n",
    "        manhattan_norm_dir = os.path.join(\n",
    "            \"metric_results\",\n",
    "            f\"{PCT_LABEL}_threshold\",\n",
    "            \"Agglomerative_manhattan_clustering_unimputed_normalized\"\n",
    "        )\n",
    "        os.makedirs(manhattan_norm_dir, exist_ok=True)\n",
    "    \n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_man_norm,\n",
    "            filename=\"Manhattan_unimputed_normalized_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=manhattan_norm_dir\n",
    "        )\n",
    "        metrics_df_agg_man_norm.to_csv(\n",
    "            os.path.join(manhattan_norm_dir, \"Manhattan_unimputed_normalized_metrics_score.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_man_norm = get_top_clusters_with_threshold(metrics_df_agg_man_norm, SIL_THR, logger=logger)\n",
    "    \n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_man_norm = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_man_norm,\n",
    "            fisher_fn=run_fisher_test_agglomerative_manhattan,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_man_norm,\n",
    "            df_ids=unimputed_normalized[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Manhattan-Unimputed-Normalized-{PCT_LABEL}\"\n",
    "        )\n",
    "    \n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_man_norm.items():\n",
    "            fisher_csv_path = os.path.join(\n",
    "                manhattan_norm_dir,\n",
    "                f\"Manhattan_unimputed_normalized_fisher_test_{k}.csv\"\n",
    "            )\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    \n",
    "    else:\n",
    "        metrics_df_agg_man_norm = dist_man_norm = top_clust_agg_man_norm = all_fisher_agg_man_norm = None\n",
    "\n",
    "\n",
    "# ==================================================================================\n",
    "#    Agglomerative Clustering using Manhattan Distance matrix (Imputed vector)\n",
    "# ==================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Manhattan distance matrix using imputed vector\n",
    "    if utils.should_run(\"AGG_MANHATTAN_IMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_man_imputed, dist_man = run_agglomerative_metrics_manhattan_imputed(\n",
    "            df_raw=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        manhattan_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_manhattan_clustering_imputed\")\n",
    "        os.makedirs(manhattan_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_man_imputed,\n",
    "            filename=\"Manhattan_imputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=manhattan_dir\n",
    "        )\n",
    "        metrics_df_agg_man_imputed.to_csv(os.path.join(manhattan_dir, \"Manhattan_imputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_man = get_top_clusters_with_threshold(metrics_df_agg_man_imputed, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_man = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_man,\n",
    "            fisher_fn=run_fisher_test_agglomerative_manhattan_imputed,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_man,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Manhattan-Imputed-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_man.items():\n",
    "            fisher_csv_path = os.path.join(manhattan_dir, f\"Manhattan_imputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_man_imputed = dist_man = top_clust_agg_man = all_fisher_agg_man = None\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "#    Agglomerative Clustering using Mahalanobis Distance matrix (Unimputed vector)\n",
    "# =================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Mahalanobis distance matrix using unimputed vector\n",
    "    if utils.should_run(\"AGG_MAHALANOBIS_UNIMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_mah, dist_mah = run_agglomerative_metrics_mahalanobis(\n",
    "            df_raw=pivot_percent,           # Unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        mahalanobis_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_mahalanobis_clustering_unimputed\")\n",
    "        os.makedirs(mahalanobis_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_mah,\n",
    "            filename=\"Mahalanobis_unimputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=mahalanobis_dir\n",
    "        )\n",
    "        metrics_df_agg_mah.to_csv(os.path.join(mahalanobis_dir, \"Mahalanobis_unimputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_mah = get_top_clusters_with_threshold(metrics_df_agg_mah, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_mah = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_mah,\n",
    "            fisher_fn=run_fisher_test_agglomerative_mahalanobis,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_mah,\n",
    "            df_ids=pivot_percent[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Mahalanobis-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "         # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_mah.items():\n",
    "            fisher_csv_path = os.path.join(mahalanobis_dir, f\"Mahalanobis_unimputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_mah = dist_mah = top_clust_agg_mah = all_fisher_agg_mah = None\n",
    "\n",
    "\n",
    "# ===============================================================================================\n",
    "#    Agglomerative Clustering using Mahalanobis Distance matrix (Normalized unimputed vector)\n",
    "# ===============================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Mahalanobis distance matrix using normalized unimputed vector\n",
    "    if utils.should_run(\"AGG_MAHANONBIS_UNIMPUTED_NORMALIZED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_mah_nor, dist_mah_nor = run_agglomerative_metrics_mahalnobis_unimputed_normalized(\n",
    "            df_raw=unimputed_normalized,           # Normalized unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        mahalanobis_norm_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_mahalanobis_unimputed_normalized\")\n",
    "        os.makedirs(mahalanobis_norm_dir, exist_ok=True)\n",
    "    \n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_mah_nor,\n",
    "            filename=\"Mahalanobis_normalized_unimputed_metrics.png\",\n",
    "            logger=logger,\n",
    "            folder=mahalanobis_norm_dir\n",
    "        )\n",
    "        metrics_df_mah_nor.to_csv(os.path.join(mahalanobis_norm_dir, \"Mahalnobis_normalized_unimputed_metrics.csv\"), index=False)\n",
    "    \n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_cluster_mah_nor = get_top_clusters_with_threshold(metrics_df_mah_nor, SIL_THR, logger=logger)\n",
    "    \n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_mah_nor = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_cluster_mah_nor,\n",
    "            fisher_fn=run_fisher_test_agglomerative_mahalanobis,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_mah_nor,\n",
    "            df_ids=unimputed_normalized[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglomerative-mahalanobis-Normalized-Unimputed-{PCT_LABEL}\"\n",
    "        )\n",
    "    \n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, df_fisher in all_fisher_mah_nor.items():\n",
    "            fisher_csv_path = os.path.join(mahalanobis_norm_dir, f\"mahalanobis_normalized_unimputed_fisher_k{k}.csv\")\n",
    "            df_fisher.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_mah_nor = dist_mah_nor = top_cluster_mah_nor = all_fisher_mah_nor = None\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "#    Agglomerative Clustering using Mahalanobis Distance matrix (Imputed vector)\n",
    "# ================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Mahalanobis distance matrix using imputed vector\n",
    "    if utils.should_run(\"AGG_MAHALANOBIS_IMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_mah_imputed, dist_mah = run_agglomerative_metrics_mahalanobis_imupted(\n",
    "            df_raw=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        mahalanobis_imp_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_mahalanobis_clustering_imputed\")\n",
    "        os.makedirs(mahalanobis_imp_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_mah_imputed,\n",
    "            filename=\"Mahalanobis_imputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=mahalanobis_imp_dir\n",
    "        )\n",
    "        metrics_df_agg_mah_imputed.to_csv(os.path.join(mahalanobis_imp_dir, \"Mahalanobis_imputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_mah = get_top_clusters_with_threshold(metrics_df_agg_mah_imputed, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_mah = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_mah,\n",
    "            fisher_fn=run_fisher_test_agglomerative_mahalanobis_imputed,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_mah,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Mahalanobis-Imputed-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_mah.items():\n",
    "            fisher_csv_path = os.path.join(mahalanobis_imp_dir, f\"Mahalanobis_imputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_mah_imputed = dist_mah = top_clust_agg_mah = all_fisher_agg_mah = None\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "#    Agglomerative Clustering using Cosine Distance matrix (Unimputed vector)\n",
    "# ===============================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Cosine distance matrix using unimputed vector\n",
    "    if utils.should_run(\"AGG_COSINE_UNIMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_cos, dist_cos = run_agglomerative_metrics_cosine(\n",
    "            df_raw=pivot_percent,           # Unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        cosine_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_cosine_clustering_unimputed\")\n",
    "        os.makedirs(cosine_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_cos,\n",
    "            filename=\"Cosine_unimputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=cosine_dir\n",
    "        )\n",
    "        metrics_df_agg_cos.to_csv(os.path.join(cosine_dir, \"Cosine_unimputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_cos = get_top_clusters_with_threshold(metrics_df_agg_cos, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_cos = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_cos,\n",
    "            fisher_fn=run_fisher_test_agglomerative_cosine,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_cos,\n",
    "            df_ids=pivot_percent[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Cosine-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_cos.items():\n",
    "            fisher_csv_path = os.path.join(cosine_dir, f\"Cosine_unimputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_cos = dist_cos = top_clust_agg_cos = all_fisher_agg_cos = None\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "#    Agglomerative Clustering using Cosine Distance matrix (Normalized unimputed vector)\n",
    "# =========================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Cosine distance matrix using normalized unimputed vector\n",
    "    if utils.should_run(\"AGG_COSINE_UNIMPUTED_NORMALIZED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_cos_norm, dist_cos_norm = run_agglomerative_metrics_cosine_unimputed_normalized(\n",
    "            df_raw=unimputed_normalized,           # Normalized unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        cosine_norm_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_cosine_clustering_unimputed_normalized\")\n",
    "        os.makedirs(cosine_norm_dir, exist_ok=True)\n",
    "    \n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_cos_norm,\n",
    "            filename=\"Cosine_unimputed_normalized_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=cosine_norm_dir\n",
    "        )\n",
    "        metrics_df_agg_cos_norm.to_csv(os.path.join(cosine_norm_dir, \"Cosine_unimputed_normalized_metrics_score.csv\"), index=False)\n",
    "    \n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_cos_norm = get_top_clusters_with_threshold(metrics_df_agg_cos_norm, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_cos_norm = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_cos_norm,\n",
    "            fisher_fn=run_fisher_test_agglomerative_cosine,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_cos_norm,\n",
    "            df_ids=unimputed_normalized[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Cosine-Unimputed-Normalized-{PCT_LABEL}\"\n",
    "        )\n",
    "    \n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_cos_norm.items():\n",
    "            fisher_csv_path = os.path.join(cosine_norm_dir, f\"Cosine_unimputed_normalized_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    \n",
    "    else:\n",
    "        metrics_df_agg_cos_norm = dist_cos_norm = top_clust_agg_cos_norm = all_fisher_agg_cos_norm = None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#    Agglomerative Clustering using Cosine Distance matrix (Imputed vector)\n",
    "# ==============================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Cosine distance matrix using imputed data\n",
    "    if utils.should_run(\"AGG_COSINE_IMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_cos, dist_cos = run_agglomerative_metrics_cosine_imputed(\n",
    "            df_raw=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        cosine_imp_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_cosine_clustering_imputed\")\n",
    "        os.makedirs(cosine_imp_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_cos,\n",
    "            filename=\"Cosine_imputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=cosine_imp_dir\n",
    "        )\n",
    "        metrics_df_agg_cos.to_csv(os.path.join(cosine_imp_dir, \"Cosine_imputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_cos = get_top_clusters_with_threshold(metrics_df_agg_cos, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_cos = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_cos,\n",
    "            fisher_fn=run_fisher_test_agglomerative_cosine_imputed,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_cos,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Cosine-Imputed-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_cos.items():\n",
    "            fisher_csv_path = os.path.join(cosine_imp_dir, f\"Cosine_imputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_cos = dist_cos = top_clust_agg_cos = all_fisher_agg_cos = None\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "#    Agglomerative Clustering using Euclidean Distance matrix (Unimputed vector)\n",
    "# ================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Euclidean distance matrix using unimputed vector\n",
    "    if utils.should_run(\"AGG_EUCLID_UNIMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_eu_pre, dist_eu_pre = run_agglomerative_metrics_euclidean_precomputed(\n",
    "            df_raw=pivot_percent,           # Unimputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        euclid_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_euclidean_clustering_unimputed\")\n",
    "        os.makedirs(euclid_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_eu_pre,\n",
    "            filename=\"Euclidean_unimputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=euclid_dir\n",
    "        )\n",
    "        metrics_df_agg_eu_pre.to_csv(os.path.join(euclid_dir, \"Euclidean_unimputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_eu_pre = get_top_clusters_with_threshold(metrics_df_agg_eu_pre, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_eu_pre = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_eu_pre,\n",
    "            fisher_fn=run_fisher_test_agglomerative_euclidean_precomputed,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_eu_pre,\n",
    "            df_ids=pivot_percent[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Euclid-precomputed-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_eu_pre.items():\n",
    "            fisher_csv_path = os.path.join(euclid_dir, f\"Euclidean_unimputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_eu_pre = dist_eu_pre = top_clust_agg_eu_pre = all_fisher_agg_eu_pre = None\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================================================\n",
    "#    Agglomerative Clustering using Euclidean Distance matrix (Normalized unimputed vector)\n",
    "# ===========================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Euclidean distance matrix using normalized unimputed vector\n",
    "    if utils.should_run(\"AGG_EUCLID_UNIMPUTED_NORMALIZED\", RUN_ALL, RUN_FLAGS):\n",
    "    \n",
    "        metrics_df_euc_nor, dist_matrix = run_agglomerative_euclidean_unimputed_normalized(\n",
    "            df_raw=unimputed_normalized,  # Normalized unimputed matrix\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,  \n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        euclid_norm_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_euclidean_unimputed_normalized\")\n",
    "        os.makedirs(euclid_norm_dir, exist_ok=True)\n",
    "    \n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_euc_nor,\n",
    "            filename=\"Euclidean_unimputed_normalized_metrics.png\",\n",
    "            logger=logger,\n",
    "            folder=euclid_norm_dir\n",
    "        )\n",
    "        metrics_df_euc_nor.to_csv(os.path.join(euclid_norm_dir, \"Euclidean_unimputed_normalized_metrics.csv\"), index=False)\n",
    "    \n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clusters = get_top_clusters_with_threshold(metrics_df_euc_nor, SIL_THR, logger=logger)\n",
    "    \n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clusters,\n",
    "            fisher_fn=run_fisher_test_agglomerative_euclidean_precomputed,  # Use existing function\n",
    "            base_kwargs=dict(\n",
    "                outcome_df=cohort_data,\n",
    "                outcome_col=\"target\"\n",
    "            ),\n",
    "            dist_matrix=dist_matrix,\n",
    "            df_ids=unimputed_normalized[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglomerative_Euclidean_Unimputed_Normalized_{PCT_LABEL}\"\n",
    "        )\n",
    "    \n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, df in all_fisher.items():\n",
    "            df.to_csv(os.path.join(euclid_norm_dir, f\"Euclidean_unimputed_normalized_fisher_k{k}.csv\"), index=False)\n",
    "    \n",
    "    else:\n",
    "        metrics_df = dist_matrix = all_fisher = None\n",
    "\n",
    "\n",
    "# ================================================================================\n",
    "#    Agglomerative Clustering using Euclidean Distance matrix (Imputed vector)\n",
    "# ================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Euclidean distance matrix using imputed vector\n",
    "    if utils.should_run(\"AGG_EUCLID_IMPUTED\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_eu_imputed, dist_eu_imputed = run_agglomerative_metrics_euclidean_imputed(\n",
    "            df_raw=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        euclid_imp_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_euclidean_clustering_imputed\")\n",
    "        os.makedirs(euclid_imp_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_eu_imputed,\n",
    "            filename=\"Euclidean_imputed_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=euclid_imp_dir\n",
    "        )\n",
    "        metrics_df_agg_eu_imputed.to_csv(os.path.join(euclid_imp_dir, \"Euclidean_imputed_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_eu_imputed = get_top_clusters_with_threshold(metrics_df_agg_eu_imputed, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_eu_imputed = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_eu_imputed,\n",
    "            fisher_fn=run_fisher_test_agglomerative_euclidean_imputed,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dist_eu_imputed,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-Euclidean-Imputed-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_eu_imputed.items():\n",
    "            fisher_csv_path = os.path.join(euclid_imp_dir, f\"Euclidean_imputed_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_eu_imputed = dist_eu_imputed = top_clust_agg_eu_imputed = all_fisher_agg_eu_imputed = None\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "#    Agglomerative Clustering using DTW matrix (fast, dtaidistance)\n",
    "# ====================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed DTW distance matrix using dtaidistance\n",
    "    if utils.should_run(\"AGG_DTW_FAST\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_dtw, dtw_mat = run_agglomerative_metrics_dtw(\n",
    "            df_features=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            n_timesteps=days,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        dtw_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_clustering_DTW_Fast_imputed\")\n",
    "        os.makedirs(dtw_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_dtw,\n",
    "            filename=\"DTW_Fast_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=dtw_dir\n",
    "        )\n",
    "        metrics_df_agg_dtw.to_csv(os.path.join(dtw_dir, \"DTW_Fast_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_dtw = get_top_clusters_with_threshold(metrics_df_agg_dtw, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_dtw = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_dtw,\n",
    "            fisher_fn=run_fisher_test_agglomerative_dtw,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dtw_mat,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-DTW-{PCT_LABEL}\",\n",
    "            dist_arg_name=\"dtw_matrix\",\n",
    "            ids_arg_name=\"df_ids\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_dtw.items():\n",
    "            fisher_csv_path = os.path.join(dtw_dir, f\"DTW_Fast_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_dtw = dtw_mat = top_clust_agg_dtw = all_fisher_agg_dtw = None\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#    Agglomerative Clustering using DTW matrix (tslearn)\n",
    "# =========================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed DTW distance matrix using tslearn.cdist_dtw\n",
    "    if utils.should_run(\"AGG_DTW_TS\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_df_agg_dtw_ts, dtw_ts = run_agglomerative_metrics_dtw_tslearn(\n",
    "            df_features=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            n_timesteps=days,\n",
    "            k_values=K_RANGE,\n",
    "            df_for_space=imputed_preprocessed,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        dtw_ts_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_clustering_DTW_tslearn_imputed\")\n",
    "        os.makedirs(dtw_ts_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_df_agg_dtw_ts,\n",
    "            filename=\"DTW_tslearn_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=dtw_ts_dir\n",
    "        )\n",
    "        metrics_df_agg_dtw_ts.to_csv(os.path.join(dtw_ts_dir, \"DTW_tslearn_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_clust_agg_dtw_ts = get_top_clusters_with_threshold(metrics_df_agg_dtw_ts, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_agg_dtw_ts = utils.run_fisher_loop_precomputed(\n",
    "            top_clusters=top_clust_agg_dtw_ts,\n",
    "            fisher_fn=run_fisher_test_agglomerative_dtw_tslearn,\n",
    "            base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "            dist_matrix=dtw_ts,\n",
    "            df_ids=imputed_preprocessed[id_vars],\n",
    "            logger=logger,\n",
    "            method_label=f\"Agglo-DTW-tslearn-{PCT_LABEL}\",\n",
    "            dist_arg_name=\"dtw_matrix\",\n",
    "            ids_arg_name=\"df_ids\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_agg_dtw_ts.items():\n",
    "            fisher_csv_path = os.path.join(dtw_ts_dir, f\"DTW_tslearn_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_df_agg_dtw_ts = dtw_ts = top_clust_agg_dtw_ts = all_fisher_agg_dtw_ts = None\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "#     Agglomerative Clustering using Binary Distance Matrices (Hamming/Jaccard/Dice)\n",
    "# ======================================================================================\n",
    "\n",
    "    # Run Agglomerative clustering with precomputed Binary matrices (Hamming/Jaccard/Dice) using imputed vector\n",
    "    if utils.should_run(\"BINARY\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        binary_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Agglomerative_binary_clustering\")\n",
    "        os.makedirs(binary_dir, exist_ok=True)\n",
    "\n",
    "        # Prepare binary feature matrices and precompute distance matrices for Hamming, Jaccard, and Dice\n",
    "        bin_feats_np, bin_feats_bool_np, dist_mats_bin, binary_dataset = prepare_binary_distance_matrices(\n",
    "            filtered_common_labs_percent,           # Filtered common labs dataframe\n",
    "            set_percentage,\n",
    "            id_col=\"hadm_id\",\n",
    "            item_col=\"itemid\",\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Run agglomerative clustering with each binary distance matrix and collect evaluation metrics. Returns a dict with a DataFrame of metrics for each distance type\n",
    "        metrics_dfs_bin = run_agglomerative_metrics_binary(\n",
    "            dist_mats=dist_mats_bin,\n",
    "            feature_space_np=bin_feats_np,\n",
    "            k_values=K_RANGE,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # For each binary distance type (Hamming, Jaccard, Dice), the metrics are saved as PNG and CSV for documentation and later review\n",
    "        for name, df in metrics_dfs_bin.items():\n",
    "            utils.plot_and_save_kmeans_metrics(\n",
    "                df,\n",
    "                filename=f\"metrics_graph_binary_{name.lower()}.png\",\n",
    "                logger=logger,\n",
    "                folder=binary_dir\n",
    "            )\n",
    "            df.to_csv(os.path.join(binary_dir, f\"metrics_score_binary_{name.lower()}.csv\"), index=False)\n",
    "\n",
    "        all_fisher_agg_bin = {}\n",
    "\n",
    "        # For each distance type, determine top cluster counts by metrics and run Fisher's exact test\n",
    "        for name, df in metrics_dfs_bin.items():\n",
    "            top_clust_bin = get_top_clusters_with_threshold(df, SIL_THR, logger=logger)\n",
    "            logger.info(\"Top clusters for %s: %s\", name, top_clust_bin)\n",
    "\n",
    "            # For each chosen k, run Fisher's test to evaluate association between clusters and outcome\n",
    "            res_dict = utils.run_fisher_loop_precomputed(\n",
    "                top_clusters=top_clust_bin,\n",
    "                fisher_fn=run_fisher_test_agglomerative_binary,\n",
    "                base_kwargs=dict(outcome_df=cohort_data, outcome_col=\"target\"),\n",
    "                dist_matrix=dist_mats_bin[name],\n",
    "                df_ids=binary_dataset[[\"hadm_id\"]],\n",
    "                logger=logger,\n",
    "                method_label=f\"Binary-{name}-{PCT_LABEL}\",\n",
    "                dist_arg_name=\"dist_matrix\",\n",
    "                ids_arg_name=\"df_ids\"\n",
    "            )\n",
    "            # Collect Fisher result DataFrames per distance/k\n",
    "            all_fisher_agg_bin.update({f\"{name}_{k}\": v for k, v in res_dict.items()})\n",
    "\n",
    "            # Save each Fisher's test result as a CSV file, named by method, distance, and k\n",
    "            for k, fisher_df in res_dict.items():\n",
    "                fisher_csv_path = os.path.join(binary_dir, f\"fisher_test_binary_{name.lower()}_{k}.csv\")\n",
    "                fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        bin_feats_np = bin_feats_bool_np = dist_mats_bin = binary_dataset = metrics_dfs_bin = all_fisher_agg_bin = None\n",
    "# ==============================================\n",
    "#   DBSCAN clustering using 'euclidean' metric\n",
    "# ==============================================\n",
    "\n",
    "    # Run DBSCAN clustering using 'euclidean' metric & imupted vector\n",
    "    if utils.should_run(\"DBSCAN_EU\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        dbscan_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"DBSCAN_clustering_euclidean_imputed\")\n",
    "        os.makedirs(dbscan_dir, exist_ok=True)\n",
    "\n",
    "        # Run the DBSCAN clustering block tailored for Euclidean metric using the imputed feature matrix. Pass all required parameters including clustering hyperparameters and paths\n",
    "        dbscan_metrics_eu, dbscan_labels_eu, fisher_eu_dbscan = utils.run_dbscan_block_euclidean(\n",
    "            df_features=imputed_preprocessed,           # Imputed vector\n",
    "            id_vars=id_vars,\n",
    "            k_for_elbow=5,\n",
    "            eps=20,\n",
    "            min_samples=5,\n",
    "            outcome_df=cohort_data,\n",
    "            outcome_col=\"target\",\n",
    "            logger=logger,\n",
    "            prefix=f\"DBSCAN-Euclidean-{PCT_LABEL}\",\n",
    "            output_folder=dbscan_dir\n",
    "                )\n",
    "\n",
    "        # If clustering metrics are produced (not None), save metrics CSV file in the output folder\n",
    "        if dbscan_metrics_eu is not None:\n",
    "            # Note: metric plots are handled inside utils.run_dbscan_block_euclidean\n",
    "            dbscan_metrics_eu.to_csv(os.path.join(dbscan_dir, \"DBSCAN_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # If Fisher’s test results are produced (not None), save each cluster count’s Fisher result as CSV\n",
    "        if fisher_eu_dbscan is not None:\n",
    "            for k, fisher_df in fisher_eu_dbscan.items():\n",
    "                fisher_csv_path = os.path.join(dbscan_dir, f\"DBSCAN_fisher_test_{k}.csv\")\n",
    "                fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        dbscan_metrics_eu = dbscan_labels_eu = fisher_eu_dbscan = None\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# #    Spectral Clustering\n",
    "# # =========================\n",
    "\n",
    "#      # Run HMM-based feature representation and Spectral clustering using imputed vector\n",
    "#     if utils.should_run(\"SPECTRAL\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "#         hmm_feats, ids_df = build_hmm_state_features(\n",
    "#             df_features=imputed_preprocessed,           # Imputed vector\n",
    "#             id_vars=id_vars,\n",
    "#             n_timesteps=days,\n",
    "#             n_hmm_states=3,\n",
    "#             logger=logger\n",
    "#         )\n",
    "\n",
    "#         # Create per-method/per-threshold output directory\n",
    "#         spectral_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Spectral_clustering_imputed\")\n",
    "#         os.makedirs(spectral_dir, exist_ok=True)\n",
    "\n",
    "#         # The metrics are saved as PNG and CSV for documentation and later review\n",
    "#         metrics_df_spec, top_clust_spec = utils.spectral_metrics_block(\n",
    "#             feature_matrix=hmm_feats,\n",
    "#             k_values=K_RANGE,\n",
    "#             affinity=\"nearest_neighbors\",\n",
    "#             plot_filename=f\"Spectral_metrics_graph.png\",\n",
    "#             sil_threshold=SIL_THR,\n",
    "#             logger=logger,\n",
    "#             folder=os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"spectral_clustering_imputed\")  # pass folder here\n",
    "#         )\n",
    "\n",
    "#         metrics_csv_path = os.path.join(spectral_dir, f\"Spectral_metrics_score.csv\")\n",
    "#         metrics_df_spec.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "#         # Run Fisher's Exact test for each selected cluster count\n",
    "#         all_fisher_spec = utils.run_fisher_loop_spectral(\n",
    "#             top_clusters=top_clust_spec,\n",
    "#             feature_matrix=hmm_feats,\n",
    "#             ids_df=ids_df,\n",
    "#             outcome_df=cohort_data,\n",
    "#             outcome_col=\"target\",\n",
    "#             logger=logger,\n",
    "#             method_label=f\"Spectral-{PCT_LABEL}\"\n",
    "#         )\n",
    "\n",
    "#         # Save Fisher's test results CSVs\n",
    "#         for k, fisher_df in all_fisher_spec.items():\n",
    "#             fisher_csv_path = os.path.join(spectral_dir, f\"Spectral_fisher_test_{k}.csv\")\n",
    "#             fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "\n",
    "    \n",
    "#     else:\n",
    "#         hmm_feats = ids_df = metrics_df_spec = top_clust_spec = all_fisher_spec = None\n",
    "\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Spectral Clustering \n",
    "# ========================\n",
    "\n",
    "    if utils.should_run(\"SPECTRAL\", RUN_ALL, RUN_FLAGS):\n",
    "    \n",
    "        # Generate HMM features\n",
    "        hmm_feats, ids_df = build_hmm_state_features(\n",
    "            df_features=imputed_preprocessed,\n",
    "            id_vars=id_vars,\n",
    "            n_timesteps=days,\n",
    "            n_hmm_states=3,\n",
    "            logger=logger\n",
    "        )\n",
    "    \n",
    "        spectral_folder = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \"Spectral_clustering_imputed\")\n",
    "        os.makedirs(spectral_folder, exist_ok=True)\n",
    "    \n",
    "        # Compute clustering metrics for range of k\n",
    "        metrics_df, top_clusters = utils.spectral_metrics_block(\n",
    "            hmm_feats,\n",
    "            k_values=K_RANGE,\n",
    "            affinity=\"nearest_neighbors\",\n",
    "            plot_filename=\"Spectral_metrics_graph.png\",\n",
    "            sil_threshold=SIL_THR,\n",
    "            logger=logger,\n",
    "            folder=spectral_folder\n",
    "        )\n",
    "    \n",
    "        # Save metrics CSV\n",
    "        metrics_csv_path = os.path.join(spectral_folder, \"Spectral_metrics_score.csv\")\n",
    "        metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "        logger.info(f\"Saved spectral clustering metrics to {metrics_csv_path}\")\n",
    "    \n",
    "        # Run Fisher tests for top clusters\n",
    "        all_fisher = utils.run_fisher_loop_spectral(\n",
    "            top_clusters=top_clusters,\n",
    "            feature_matrix=hmm_feats,\n",
    "            ids_df=ids_df,\n",
    "            outcome_df=cohort_data,\n",
    "            outcome_col='target',\n",
    "            logger=logger,\n",
    "            method_label=f\"Spectral-{PCT_LABEL}\"\n",
    "        )\n",
    "    \n",
    "        # Save Fisher results and cluster assignments per k\n",
    "        for key, fisher_df in all_fisher.items():\n",
    "            # Save Fisher results CSV\n",
    "            fisher_csv_path = os.path.join(spectral_folder, f\"Spectral_fisher_test_{key}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "            logger.info(f\"Saved Fisher test results: {fisher_csv_path}\")\n",
    "    \n",
    "            # Extract cluster count k from key string like 'silhouette_k=12'\n",
    "            match = re.search(r'k=(\\d+)', key)\n",
    "            if not match:\n",
    "                logger.warning(f\"Could not parse cluster count k from key '{key}', skipping cluster assignment save.\")\n",
    "                continue\n",
    "            k = int(match.group(1))\n",
    "    \n",
    "            # Compute cluster assignments with SpectralClustering for this k\n",
    "            clustering_model = SpectralClustering(\n",
    "                n_clusters=k,\n",
    "                affinity=\"nearest_neighbors\",\n",
    "                random_state=42\n",
    "            )\n",
    "            labels = clustering_model.fit_predict(hmm_feats)\n",
    "    \n",
    "            # Save assignments CSV\n",
    "            cluster_assignments = pd.DataFrame({\n",
    "                'hadm_id': ids_df['hadm_id'],\n",
    "                'cluster_label': labels\n",
    "            })\n",
    "            assignment_csv_path = os.path.join(spectral_folder, f\"Spectral_cluster_assignments_k={k}.csv\")\n",
    "            cluster_assignments.to_csv(assignment_csv_path, index=False)\n",
    "            logger.info(f\"Saved spectral cluster assignments for k={k} to {assignment_csv_path}\")\n",
    "    \n",
    "    else:\n",
    "        hmm_feats = None\n",
    "        ids_df = None\n",
    "        metrics_df = None\n",
    "        top_clusters = None\n",
    "        all_fisher = None\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "#    K‑MEDOIDS Clustering using 'euclidean' metric\n",
    "# ==================================================\n",
    "\n",
    "    # Run K‑MEDOIDS clustering using Euclidean metric\n",
    "    if utils.should_run(\"KMEDOIDS_EUCLID\", RUN_ALL, RUN_FLAGS):\n",
    "\n",
    "        metrics_kmed_eu = run_kmedoids_metrics(\n",
    "            imputed_preprocessed,           # Imputed vector\n",
    "            id_vars,\n",
    "            k_values=K_RANGE,\n",
    "            metric=\"euclidean\",\n",
    "            precomputed=False,\n",
    "            logger=logger\n",
    "        )\n",
    "\n",
    "        # Create per-method/per-threshold output directory\n",
    "        kmed_dir = os.path.join(\"metric_results\", f\"{PCT_LABEL}_threshold\", \" K_MEDOIDS_clustering_imputed\")\n",
    "        os.makedirs(kmed_dir, exist_ok=True)\n",
    "\n",
    "        # The metrics are saved as PNG and CSV for documentation and later review\n",
    "        utils.plot_and_save_kmeans_metrics(\n",
    "            metrics_kmed_eu,\n",
    "            filename=\"K_MEDOIDS_metrics_graph.png\",\n",
    "            logger=logger,\n",
    "            folder=kmed_dir\n",
    "        )\n",
    "        metrics_kmed_eu.to_csv(os.path.join(kmed_dir, \"K_MEDOIDS_metrics_score.csv\"), index=False)\n",
    "\n",
    "        # Top clusters per metric (Silhouette score, Calinski-Harabasz index and Davies-Bouldin index)\n",
    "        top_kmed_eu = get_top_clusters_with_threshold(metrics_kmed_eu, SIL_THR, logger=logger)\n",
    "\n",
    "        # Run Fisher's Exact test for each selected cluster count\n",
    "        all_fisher_kmed_eu = utils.run_fisher_loop(\n",
    "            top_clusters=top_kmed_eu,\n",
    "            fisher_fn=run_fisher_test_kmedoids,\n",
    "            base_kwargs=dict(\n",
    "                df_features=imputed_preprocessed,\n",
    "                id_vars=id_vars,\n",
    "                outcome_df=cohort_data,\n",
    "                outcome_col=\"target\",\n",
    "                metric=\"euclidean\",\n",
    "                precomputed=False,\n",
    "                random_state=42\n",
    "            ),\n",
    "            logger=logger,\n",
    "            method_label=f\"KMedoids-EU-{PCT_LABEL}\"\n",
    "        )\n",
    "\n",
    "        # Save Fisher's test results CSVs\n",
    "        for k, fisher_df in all_fisher_kmed_eu.items():\n",
    "            fisher_csv_path = os.path.join(kmed_dir, f\"K_MEDOIDS_fisher_test_{k}.csv\")\n",
    "            fisher_df.to_csv(fisher_csv_path, index=False)\n",
    "    else:\n",
    "        metrics_kmed_eu = top_kmed_eu = all_fisher_kmed_eu = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4BRxKhgxQGxB",
   "metadata": {
    "id": "4BRxKhgxQGxB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 75.0 %\n",
      "\n",
      "Flags set: {'KMEANS': False, 'AGG_EUCLID': False, 'AGG_MANHATTAN_UNIMPUTED': True, 'AGG_MANHATTAN_UNIMPUTED_NORMALIZED': False, 'AGG_MANHATTAN_IMPUTED': False, 'AGG_COSINE_UNIMPUTED': False, 'AGG_COSINE_UNIMPUTED_NORMALIZED': False, 'AGG_COSINE_IMPUTED': False, 'AGG_MAHALANOBIS_UNIMPUTED': False, 'AGG_MAHANONBIS_UNIMPUTED_NORMALIZED': False, 'AGG_MAHALANOBIS_IMPUTED': False, 'AGG_EUCLID_UNIMPUTED': False, 'AGG_EUCLID_UNIMPUTED_NORMALIZED': False, 'AGG_EUCLID_IMPUTED': False, 'AGG_DTW_FAST': False, 'AGG_DTW_TS': False, 'BINARY': False, 'DBSCAN_EU': False, 'SPECTRAL': False, 'KMEDOIDS_EUCLID': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Python’s \"importlib\" built-in library enables us to reload an updated module, such as hyperparameters.py, without restarting the kernel. This allows\n",
    "us to immediately apply and test changes made in configuration or utility files directly within our main script or notebook, streamlining iterative\n",
    "development and experimentation.\n",
    "\n",
    "'''\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import hyperparameters\n",
    "import clustering\n",
    "import clustering_analysis\n",
    "importlib.reload(hyperparameters)      # Reload the hyperparameters module to get latest changes\n",
    "importlib.reload(utils)                # Reload utils.py to reflect any function updates\n",
    "importlib.reload(clustering)           # Reload clustering functions to incorporate any changes in clustering.py code\n",
    "importlib.reload(clustering_analysis)   \n",
    "from clustering import *               # Import all updated clustering functions into the current namespace\n",
    "from hyperparameters import *          # Import all updated hyperparameter settings and flags\n",
    "from clustering_analysis import *\n",
    "\n",
    "\n",
    "'''\n",
    "  Print the current lab inclusion threshold percentage (set_percentage) after reload. This confirms what coverage threshold for lab features\n",
    "  the pipeline will use in this run.\n",
    "'''\n",
    "print(f\"Threshold: {set_percentage*100} %\\n\")  # Getting status of current threshold value after reloading utils.py module\n",
    "\n",
    "'''\n",
    "  Print the dictionary RUN_FLAGS controlling which clustering methods will be executed. Each key corresponds to a clustering algorithm,\n",
    "  with True/False indicating whether it will run. This helps verify that the pipeline will only run the selected clustering blocks as per\n",
    "  the current configuration\n",
    "'''\n",
    "print(f\"Flags set: {RUN_FLAGS}\\n\")       # Print which clustering blocks will run ('True') after reloading utils.py module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vEyoCw8KQGqQ",
   "metadata": {
    "id": "vEyoCw8KQGqQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 23:05:01,356 - [UTILS] - INFO - All required files for the selected threshold, 'filtered_common_labs', 'unimputed_vector', 'feature_matrix', unimputed_normalized_vector and 'imputed_vector' are found in 'preprocessing_data' folder. Therefore, we are going to use already stored files for further processing.\n",
      "\n",
      "2025-09-08 23:05:01,461 - [ClusterAnalysis] - INFO - Since Manhattan distance matrix on unimputed data for the given threshold is already computed, therfore loading cached Manhattan distance matrix from path distance_matrices\\75pct_threshold\\manhattan_distance_unimputed_75.npy\n",
      "\n",
      "2025-09-08 23:05:01,598 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=2 ...\n",
      "2025-09-08 23:05:02,467 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=3 ...\n",
      "2025-09-08 23:05:03,219 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=4 ...\n",
      "2025-09-08 23:05:03,790 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=5 ...\n",
      "2025-09-08 23:05:04,375 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=6 ...\n",
      "2025-09-08 23:05:04,968 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=7 ...\n",
      "2025-09-08 23:05:05,549 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=8 ...\n",
      "2025-09-08 23:05:06,136 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=9 ...\n",
      "2025-09-08 23:05:06,722 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=10 ...\n",
      "2025-09-08 23:05:07,295 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=11 ...\n",
      "2025-09-08 23:05:07,868 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=12 ...\n",
      "2025-09-08 23:05:08,548 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=13 ...\n",
      "2025-09-08 23:05:09,235 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=14 ...\n",
      "2025-09-08 23:05:09,968 - [ClusterAnalysis] - INFO - Agglomerative clustering on unimputed data with Manhattan distance for k=15 ...\n",
      "2025-09-08 23:05:11,326 - [ClusterAnalysis] - INFO - \n",
      "Clustering metrics plot named 'Manhattan_unimputed_metrics_graph.png' saved to the path metric_results\\75pct_threshold\\Agglomerative_manhattan_clustering_unimputed\\Manhattan_unimputed_metrics_graph.png\n",
      "\n",
      "2025-09-08 23:05:11,331 - [ClusterAnalysis] - INFO - \n",
      "Clustering result for all three used metrics:\n",
      "     k  silhouette  calinski_harabasz  davies_bouldin\n",
      "0    2    0.624259          11.820962        0.261581\n",
      "1    3    0.585973          21.765309        0.536040\n",
      "2    4    0.571148          22.082365        0.795210\n",
      "3    5    0.547849          19.910598        0.711884\n",
      "4    6    0.536012          16.446237        0.721657\n",
      "5    7    0.520305          14.237473        0.730631\n",
      "6    8    0.503188          12.642441        0.663281\n",
      "7    9    0.468577          12.651343        0.629917\n",
      "8   10    0.454206          11.465810        0.631361\n",
      "9   11    0.442523          10.886959        0.551605\n",
      "10  12    0.426495          11.230190        0.956790\n",
      "11  13    0.411842          10.545282        1.032047\n",
      "12  14    0.408393           9.966285        1.013892\n",
      "13  15    0.395954          11.736447        1.078648\n",
      "\n",
      "2025-09-08 23:05:11,338 - [ClusterAnalysis] - INFO - Running Fisher’s Exact Test on attained Top-3 clusters...\n",
      "\n",
      "2025-09-08 23:05:11,942 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for silhouette k=2:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     1.000000           1.000000                         False\n",
      "1        1     0.052137           0.104273                         False\n",
      "\n",
      "2025-09-08 23:05:12,545 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for silhouette k=3:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.992156            1.00000                         False\n",
      "1        1     0.052137            0.15641                         False\n",
      "2        2     1.000000            1.00000                         False\n",
      "\n",
      "2025-09-08 23:05:13,155 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for silhouette k=4:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.964668           1.000000                         False\n",
      "1        1     1.000000           1.000000                         False\n",
      "2        2     1.000000           1.000000                         False\n",
      "3        3     0.052137           0.208546                         False\n",
      "\n",
      "2025-09-08 23:05:13,695 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for calinski_harabasz k=4:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.964668           1.000000                         False\n",
      "1        1     1.000000           1.000000                         False\n",
      "2        2     1.000000           1.000000                         False\n",
      "3        3     0.052137           0.208546                         False\n",
      "\n",
      "2025-09-08 23:05:14,200 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for calinski_harabasz k=3:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.992156            1.00000                         False\n",
      "1        1     0.052137            0.15641                         False\n",
      "2        2     1.000000            1.00000                         False\n",
      "\n",
      "2025-09-08 23:05:14,784 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for calinski_harabasz k=5:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.952211           1.000000                         False\n",
      "1        1     1.000000           1.000000                         False\n",
      "2        2     1.000000           1.000000                         False\n",
      "3        3     0.052137           0.260683                         False\n",
      "4        4     1.000000           1.000000                         False\n",
      "\n",
      "2025-09-08 23:05:15,271 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for davies_bouldin k=2:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     1.000000           1.000000                         False\n",
      "1        1     0.052137           0.104273                         False\n",
      "\n",
      "2025-09-08 23:05:15,777 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for davies_bouldin k=3:\n",
      "   Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0        0     0.992156            1.00000                         False\n",
      "1        1     0.052137            0.15641                         False\n",
      "2        2     1.000000            1.00000                         False\n",
      "\n",
      "2025-09-08 23:05:16,251 - [ClusterAnalysis] - INFO - Agglo-Manhattan-75pct Fisher results for davies_bouldin k=11:\n",
      "    Cluster  Raw_p_value  Corrected_p_value  Significant_after_correction\n",
      "0         0     0.890791           1.000000                         False\n",
      "1         1     1.000000           1.000000                         False\n",
      "2         2     1.000000           1.000000                         False\n",
      "3         3     1.000000           1.000000                         False\n",
      "4         4     1.000000           1.000000                         False\n",
      "5         5     1.000000           1.000000                         False\n",
      "6         6     1.000000           1.000000                         False\n",
      "7         7     1.000000           1.000000                         False\n",
      "8         8     0.052137           0.573502                         False\n",
      "9         9     1.000000           1.000000                         False\n",
      "10       10     1.000000           1.000000                         False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#           ============================================ MAIN DRIVER ================================================\n",
    "\n",
    "'''\n",
    "Description of the data files:\n",
    "- cohort_data: DataFrame containing patient admission metadata and clinical outcomes, loaded from cached pickle files or raw data.\n",
    "\n",
    "- labevents_data: DataFrame with individual laboratory event records and timestamps, linked to patient admissions,loaded from cached pickle files or raw data.\n",
    "\n",
    "- filtered_common_labs_percent: DataFrame of lab events filtered by coverage threshold, retaining frequently measured labs; used as input for Agglomerative Clustering \n",
    "                                with Binary Distance Matrices.\n",
    "\n",
    "- pivot_percent (Unimputed vector): Wide-format unimputed vector (admissions x lab-day features) representing raw measurements before Normalization/Linear Interpolation/\n",
    "                                    KNN Imputation; used for clustering methods based on pre-computed distance matrices such as Agglomerative Clustering with Manhattan,\n",
    "                                    Cosine, Mahalanobis and Euclidean metrics.\n",
    "\n",
    "- feature_matrix_percent: 2D NumPy array derived from pivot_percent vector, serving as input to compute distance matrices for selected algorithms.\n",
    "\n",
    "- unimputed_normalized: (Unimputed normalized vector): Wide-format normalized unimputed vector (admissions x lab-day features) representing raw measurements before  \n",
    "                                                       Linear Interpolation/KNN Imputation; used for clustering methods based on pre-computed distance matrices such as \n",
    "                                                       Agglomerative Clustering with Manhattan, Cosine, Mahalanobis and Euclidean metrics.\n",
    "\n",
    "- imputed_preprocessed (Imputed vector): Fully preprocessed vector with normalized,interpolated and imputed values, serving as primary input for clustering algorithms\n",
    "                                         like KMeans, KMedoids (Euclidean),Agglomerative Clustering using DTW matrix, DBSCAN, and Spectral Clustering.\n",
    "\n",
    "- id_vars: List of columns (e.g., ['hadm_id', 'subject_id']) serving as unique admission identifiers throughout the pipeline.\n",
    "\n",
    "- PCT_LABEL: String identifier reflecting the lab coverage percentage threshold, used consistently for caching, file naming, and reproducibility.\n",
    "\n",
    "'''\n",
    "\n",
    "def main():\n",
    "\n",
    "    '''\n",
    "    Generate a label reflecting the feature inclusion threshold (e.g., \"75pct\") for consistent caching and output naming.\n",
    "    '''\n",
    "    PCT_LABEL = f\"{int(set_percentage*100)}pct\"\n",
    "\n",
    "    '''\n",
    "    Execute the data loading step, retrieving or caching patient cohort and lab events datasets.\n",
    "    '''\n",
    "    cohort_data, labevents_data = load_data()\n",
    "\n",
    "    '''\n",
    "    Perform data preprocessing, including filtering, pivoting, and Normalization/Imputation/Interpolation.\n",
    "    The results include filtered lab events, unimputed and imputed feature matrices, and identifier columns.\n",
    "    '''\n",
    "    filtered_common_labs_percent, pivot_percent, feature_matrix_percent, unimputed_normalized, imputed_preprocessed, id_vars = preprocess(\n",
    "        cohort_data, labevents_data, set_percentage, days, PCT_LABEL)\n",
    "\n",
    "    '''\n",
    "    Run the clustering algorithms and evaluation pipeline, passing all preprocessed data and configuration.\n",
    "    This orchestrates multiple clustering methods, metric calculations, and statistical significance testing.\n",
    "    Note: The function accepts several data versions to accommodate method-specific requirements.\n",
    "    '''\n",
    "    run_clustering(filtered_common_labs_percent, pivot_percent, feature_matrix_percent, unimputed_normalized, imputed_preprocessed, id_vars, cohort_data, PCT_LABEL)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    Entry point to start the entire analysis pipeline end-to-end\n",
    "    '''\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c23b0-739f-4f9d-bb72-898b99c6d971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a2c96-c8b2-4766-8978-272fb97ff776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd8e9c-b394-49b6-ab5f-229a1fb09c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:timeseries_env]",
   "language": "python",
   "name": "conda-env-timeseries_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
